{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8985c39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#this is the code used on google colab to finetune tiny-llma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efd750",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes transformers peft accelerate datasets\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525bcf5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gcsfs==2025.3.0\n",
    "!pip install fsspec==2025.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ce4ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Upload the file\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Check if the file was uploaded successfully\n",
    "if 'CameroonLaw.txt' in uploaded:\n",
    "    print(\"CameroonLaw.txt uploaded successfully!\")\n",
    "    # Verify the file exists in the current directory (which is /content/ in Colab)\n",
    "    if os.path.exists('CameroonLaw.txt'):\n",
    "        print(\"CameroonLaw.txt found in the current directory.\")\n",
    "    else:\n",
    "        print(\"CameroonLaw.txt not found in the current directory after upload.\")\n",
    "else:\n",
    "    print(\"CameroonLaw.txt was not found in the uploaded files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da7624",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Try with a custom device_map to force all layers to GPU\n",
    "device_map = {\"\": 0}  # This forces all layers to GPU 0\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,  # Use our custom device_map\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    model_max_length=256,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336cb68",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import transformers\n",
    "\n",
    "# Load the dataset from the text file\n",
    "# This returns a DatasetDict, typically with a 'train' split\n",
    "loaded_dataset = load_dataset('text', data_files='CameroonLaw.txt')\n",
    "\n",
    "# loaded_dataset is a DatasetDict. We want to split this into train and test.\n",
    "# The train_test_split method works directly on a DatasetDict\n",
    "# and will apply the split to each existing split (in this case, 'train').\n",
    "# The result will be a new DatasetDict with 'train' and 'test' keys.\n",
    "dataset = loaded_dataset['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Print the type of the dataset variable to verify it's a DatasetDict after splitting\n",
    "# Accessing dataset['train'] or dataset['test'] will give you a Dataset object\n",
    "print(f\"Type of dataset after splitting: {type(dataset)}\")\n",
    "print(f\"Type of dataset['train'] after splitting: {type(dataset['train'])}\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        #label_names=[\"labels\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "# tokenized_dataset should be created by mapping the tokenize_function to the dataset object\n",
    "# Since dataset is now a DatasetDict with 'train' and 'test' keys,\n",
    "# the map method applied to the DatasetDict will apply the function to both splits.\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"Dataset tokenization mapping applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012bafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57100f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive again with force_remount=True to ensure a new prompt\n",
    "drive.mount('/content/drive/mount', force_remount=True)\n",
    "# The rest of your code to load dataset, tokenize, setup trainer, and train\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "\n",
    "# Load Tokenizer\n",
    "# Note: You might want to load the tokenizer from the model_id again\n",
    "# if you are using a fresh environment after mounting the drive.\n",
    "# Assuming 'model_id' is still in scope from a previous cell.\n",
    "# If not, you'll need to define it again: model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Critical for padding in causal LM\n",
    "\n",
    "# Load Plain Text Dataset\n",
    "# Ensure 'CameroonLaw.txt' is accessible from the new mount if it's not in /content\n",
    "# For example, if it's in a specific folder in your new Google Drive:\n",
    "# data_files=\"/content/drive/MyDrive/path/to/CameroonLaw.txt\"\n",
    "dataset = load_dataset(\"text\", data_files=\"/content/drive/MyDrive/CameroonLaw.txt\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1)  # Split into train/test\n",
    "\n",
    "# Tokenization (No labels needed)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training Arguments (No label_names required)\n",
    "training_args = TrainingArguments(\n",
    "    # Update the output directory path if needed based on the new mount\n",
    "    output_dir=\"/content/drive/MyDrive/tinyllama-checkpoint\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    resume_from_checkpoint=True,\n",
    "    # No need for label_names in unsupervised LM tasks\n",
    ")\n",
    "\n",
    "# Trainer for Causal LM\n",
    "# Ensure 'model' is still in scope from a previous cell.\n",
    "# If not, you'll need to reload the model as well.\n",
    "trainer = Trainer(\n",
    "    model=model, # Assuming 'model' object is available from a previous cell\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForLanguageModeling(\n",
    "        tokenizer, mlm=False  # mlm=False for causal LM\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save final model (adapter only)\n",
    "model.save_pretrained(\"/content/drive/MyDrive/tinyllama-final-lora\")\n",
    "print(\"Training completed! LoRA adapter saved.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
